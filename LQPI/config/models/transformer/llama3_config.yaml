# Model Configuration for Quantum Field Personality Experiments with Llama 3.2
# This file configures the language model parameters and settings.

# Model selection
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"  # Change to 70B version if needed
  revision: "main"
  family: "llama"
  dtype: "float32"  
  device: "auto"
  use_cache: true
  compile: false

# Tokenizer configuration
tokenizer:
  use_fast: true                           # Whether to use fast tokenizer implementation
  padding_side: "left"                     # Padding side (left or right)
  truncation_side: "right"                 # Truncation side (left or right)
  max_length: 4096                         # Maximum sequence length - Llama supports longer context

# Generation parameters
generation:
  max_new_tokens: 150                      # Maximum number of tokens to generate
  min_new_tokens: null                     # Minimum number of tokens to generate
  do_sample: true                          # Whether to use sampling
  temperature: 0.7                         # Sampling temperature
  top_p: 0.9                               # Top-p (nucleus) sampling
  top_k: 50                                # Top-k sampling
  repetition_penalty: 1.0                  # Repetition penalty
  no_repeat_ngram_size: 0                  # Size of n-grams to avoid repeating
  seed: null                               # Random seed for reproducibility

# Quantization settings
quantization:
  enabled: false                           # Whether to use quantization
  bits: 8                                  # Number of bits (4, 8)
  group_size: 128                          # Group size for quantization
  method: "gptq"                           # Quantization method (gptq, awq, etc.)

# Caching settings
cache:
  enabled: true                            # Whether to use caching
  dir: "cache"                             # Directory for caching

# Resource limits
resources:
  max_memory_mb: null                      # Maximum memory usage in MB (null = unlimited)
  cpu_offload: false                       # Whether to offload to CPU
  disk_offload: false                      # Whether to offload to disk
  load_in_4bit: false                      # Whether to load in 4-bit precision
  load_in_8bit: false                      # Whether to load in 8-bit precision

# Instrumentation settings (detailed)
instrumentation:
  # Attention layers to instrument
  attention_layers:
    enabled: true
    patterns:
      - "self_attn.o_proj"                 # Llama uses this pattern
      - "self_attn.dense"                  # Alternative pattern some variants might use
    sampling_rate: 2
    capture_qkv: true
    capture_attention_weights: true
  
  # MLP layers to instrument
  mlp_layers:
    enabled: true
    patterns:
      - "mlp.down_proj"                    # Primary Llama MLP pattern
      - "mlp.gate_proj"                    # Gate projection in Llama architecture
    sampling_rate: 2
  
  # Embedding layers to instrument
  embedding_layers:
    enabled: false
    patterns:
      - "embed_tokens"
    
  # Logging settings for instrumentation
  logging:
    log_shapes: true                       # Log tensor shapes
    log_values: false                      # Log actual tensor values (can be verbose)
    max_sequence_print: 20                 # Maximum sequence length to print in logs

# Llama-specific settings
llama_specific:
  rope_scaling: {                          # RoPE scaling for Llama models
    "type": "linear",
    "factor": 1.0
  }
  group_query_attention: true              # Whether model uses GQA
  sliding_window: 4096                     # Sliding window attention size